# -*- coding: utf-8 -*-
"""ha.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16JTDiMugEBOcGWaIA-Avn9XqLAowjDbc

# Heart Attack Prediction Model
- A classification problem to predict whether someone has heart attack
- [Source Data](https://www.kaggle.com/datasets/rashikrahmanpritom/heart-attack-analysis-prediction-dataset/code?sort=votes)

# Importing Libraries
"""

import os
import pickle
import numpy as np 
import pandas as pd
import seaborn as sns 
import scipy.stats as ss
import matplotlib.pyplot as plt

from sklearn.pipeline import Pipeline
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import \
    train_test_split, GridSearchCV
from sklearn.preprocessing import \
    LabelEncoder, MinMaxScaler, StandardScaler

from sklearn.linear_model import \
    LogisticRegression, LinearRegression

from sklearn.ensemble import \
    RandomForestClassifier, GradientBoostingClassifier

from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC ,SVR

from sklearn.metrics import \
    f1_score, confusion_matrix, accuracy_score, classification_report

from sklearn.metrics import ConfusionMatrixDisplay

#Cramers Function
def cramers_corrected_stat(confusion_matrix):
    """ calculate Cramers V statistic for categorial-categorial association.
        uses correction from Bergsma and Wicher,
        Journal of the Korean Statistical Society 42 (2013): 323-328
    """
    chi2 = ss.chi2_contingency(confusion_matrix)[0]
    n = confusion_matrix.sum()
    phi2 = chi2/n
    r,k = confusion_matrix.shape
    phi2corr = max(0, phi2 - ((k-1)*(r-1))/(n-1))  
    rcorr = r - ((r-1)**2)/(n-1)
    kcorr = k - ((k-1)**2)/(n-1)
    return np.sqrt(phi2corr / min( (kcorr-1), (rcorr-1)))

# constants 
## clean path!
CSV_PATH = os.path.join(os.getcwd(),'dataset','heart.csv')
MODEL_PATH = os.path.join(os.getcwd(),'model','model.pkl')
TEST_PATH = os.path.join(os.getcwd(),'dataset','test.csv')

def plot_con_graph(con_col,df,c):
  '''
  This function is meant to plot continuos data using seaborn distplot function

  Parameters
  ----------
  con : LIST
      con_col contains the name of the categorical columns.
  df : DATAFRAME
      DESCRIPTION
  c : str
      String that sets the colour of the plot

  Returns
  -------
  None.
  '''

  for i in con_col:
    plt.figure()
    sns.distplot(df[i],color='c')
    plt.show()

"""# 1) Data Loading"""

# from google.colab import drive
# drive.mount('/content/drive')

df = pd.read_csv(CSV_PATH)

df.head()

df.describe().T

df.nunique()

#checking for missing values
df.isna().sum()

"""# 2) Data Vizualization"""

#separating catagorical and continuos columns
con = ['age','trtbps','chol','thalachh','oldpeak']
cat = df.drop(labels=con, axis=1).columns

#custom = ['#FEBB9A','#B3D9FF', '#E3F8B9','#FFC0E7','#A9F3FF','#DFD8D1']

for i in cat:
  plt.figure()
  sns.countplot(df[i], palette= 'Pastel2') 
  plt.show()

"""Observation:
1. Slightly imbalance dataset, but acceptable

"""

df.groupby(['sex','output']).agg({'output':'count'}).plot(kind='bar',color= '#FEBB9A')

# 1: Male 0:Female

plot_con_graph(con,df,'#E3F8B9')

df.boxplot(figsize=(10,6))

df.describe().T

"""Observation:
1. Age ranges 29-77 
2. thall & caa hahave some hidden null data, thall 0 = null; caa 4 = null
3. One outlier in chol to be handled

# 3) Data Cleaning
1. Outliers
2. Missing Values
3. Duplicated

## 3.1 Dealing with Outliers
1. Clip the only outliers from chol col
"""

#clipping data with realistic blood pressures
df['chol'] = df['chol'].clip(126,430)

df.boxplot(figsize=(10,6))
#outliers are now treated

"""## 3.2 Dealing with Missing Values

"""

#querying the data with invalid input

df.query('thall == 0 or caa == 4')

"""Notes:
- We have a total of 7 entries with invalid inputs of thall and caa columns,
- We have 5 data with positive outputs.
- Since we have a really small datasets, we need to be ccareful of any imputation as it might skew the datasets from it's actual distribution.
- We're dropping the rows with invalid data for now.
"""

df.drop(df.query('thall == 0 or caa == 4').index,axis=0,inplace=True)

df.describe().T

# df.boxplot()

# for i in con:
#   df[i] = df[i].fillna(df[i].median())

# for i in cat:
#   df[i] = df[i].fillna(df[i].mode()[0])

# df.isna().sum()

"""##3.3 Droping Duplicates"""

df.duplicated().sum()
#no duplicate data

"""# 4) Features Selection"""

#cat = df[cat].drop(labels='output', axis=1).columns

for i in cat:
    print(i)
    matrix = pd.crosstab(df[i],df['output']).to_numpy()
    print(cramers_corrected_stat(matrix))

for i in con:
    print(i)
    lr=LogisticRegression()
    lr.fit(np.expand_dims(df[i],axis=-1),df['output'])
    print(lr.score(np.expand_dims(df[i],axis=-1),df['output']))

#checking why fbs output = 0
display(df.groupby('fbs')['fbs'].size())

fig = plt.figure(figsize=(6,6))
gs = fig.add_gridspec(1,1)
gs.update(wspace=0.3, hspace=0.15)
ax0 = fig.add_subplot(gs[0,0])

df_corr = df[con].corr().transpose()

mask = np.triu(np.ones_like(df_corr))
ax0.text(1.5,-0.1,"Correlation Matrix",fontsize=14, fontfamily='serif', color="#696969")
df_corr = df[con].corr().transpose()
sns.heatmap(df_corr,mask=mask,fmt=".2f",annot=True,cmap='coolwarm')
plt.show()

"""# 5) Pre-processing"""

#select features that has the highest coleration
X = df.drop(['output','fbs'],axis=1)
y = df['output']

X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3,random_state=413)

X.columns

"""# 6) Model Development"""

#Pipeline to find the best model
#Logistic Regression
pipeline_mms_lr = Pipeline([
                           ('Min_Max_Scaler', MinMaxScaler()),
                           ('Logistic_Classifier', LogisticRegression())
                           ]) # Pipeline([STEPS])


pipeline_ss_lr = Pipeline([
                           ('Standard_Scaler',StandardScaler()), 
                           ('Logistic_Classifier', LogisticRegression())
                           ]) 

#Random Forest
pipeline_mms_dt = Pipeline([
                           ('Min_Max_Scaler', MinMaxScaler()),
                           ('Decision_Tree_Classifier', DecisionTreeClassifier())
                           ]) 


pipeline_ss_dt = Pipeline([
                           ('Standard_Scaler',StandardScaler()), 
                           ('Decision_Tree_Classifier', DecisionTreeClassifier())
                           ]) 

#SVC
pipeline_mms_svc = Pipeline([
                            ('Min_Max_Scaler', MinMaxScaler()),
                            ('SVC_Classifier', SVC())
                            ]) 


pipeline_ss_svc = Pipeline([
                            ('Standard_Scaler', StandardScaler()),
                            ('SVC_Classifier', SVC())
                            ])
#gradient boost
pipeline_mms_gb = Pipeline([
                            ('Min_Max_Scaler',MinMaxScaler()),
                            ('GBoost_Classifier', GradientBoostingClassifier())
                            ])

pipeline_ss_gb = Pipeline([
                            ('Min_Max_Scaler',StandardScaler()),
                            ('GBoost_Classifier', GradientBoostingClassifier())
                            ])


#storing pipelines in a list
pipelines = [pipeline_mms_lr, pipeline_ss_lr,
             pipeline_mms_dt, pipeline_ss_dt, 
             pipeline_mms_svc, pipeline_ss_svc,
             pipeline_mms_gb, pipeline_ss_gb]

#fitting of data
for pipe in pipelines:
    pipe.fit(X_train, y_train)

#create a list that store model name for reporting table, to be looped and parsed as keys
model_name = ['MMS+Logistic Regression', 'SS+Logistic Regression', 
              'MMS+Decision Tree', 'SS+Decision Tree',
              'MMS+SVC','SS+SVC',
              'MMS+Gradient Boost','SS+Gradient Boost',
              ]

#dictionary to store evaluation matrics of all the pipelines
pipe_dict =  {}

best_score = 0

#model evaluation
for i, model in enumerate(pipelines):   
    y_pred = model.predict(X_test)

    pipe_dict[model_name[i]] = [accuracy_score(y_test,y_pred),
                    f1_score(y_test,y_pred, average='weighted')]

    if model.score(X_test, y_test) > best_score:
        best_score = model.score(X_test, y_test)
        best_pipeline = model_name[i]

print('The best scaling approach for Cardiovascular Disease Prediction is {} with the accuracy of {}'.
      format(best_pipeline,best_score))

"""# 7) Model Comparison Table"""

model_comparison_df = pd.DataFrame.from_dict(pipe_dict).T
model_comparison_df.columns = ['Accuracy', 'F1 Score']
#model_comparison_df = model_comparison_df.sort_values('F1 Score', ascending=False)
#model_comparison_df.style.background_gradient(cmap='coolwarm')
model_comparison_df.sort_values(by='F1 Score',ascending=False).style.highlight_max()

"""
### Gridsearch cv

"""

from sklearn.model_selection import GridSearchCV
#brute force to try out all combinations
pipeline_ss_lr = Pipeline([
                           ('Standard_Scaler',StandardScaler()), 
                           ('Logistic_Classifier', LogisticRegression())
                           ])

pipeline_ss_lr.get_params().keys()
#printing all hyperparameters for reference

grid_param = [{'Logistic_Classifier__C':[1.0,1.5,2.5],
               'Logistic_Classifier__class_weight':[None,'balanced'],
               'Logistic_Classifier__max_iter':[100,500,1000],
               'Logistic_Classifier__random_state':[1,3,4],
               'Logistic_Classifier__solver':['lbfgs','liblinear','sag']
             }]

grid_search = GridSearchCV(pipeline_ss_lr, grid_param, 
                                 cv=5, verbose=1, n_jobs=-1)

grid = grid_search.fit(X_train, y_train)
print(grid_search.score(X_test,y_test))
display(grid.best_params_)

#Model Evaluation
y_pred = grid.predict(X_test)
y_true = y_test

labels = ['No Heart Attack','Heart Attack']
cm = confusion_matrix(y_true,y_pred)
cr = classification_report(y_true,y_pred, target_names = labels)

print(cr)

disp = ConfusionMatrixDisplay(confusion_matrix=cm,display_labels =labels)
disp.plot(cmap='RdPu')
plt.rcParams['figure.figsize'] = [7, 7]
plt.show()

print(classification_report)

#saving the best model

with open(MODEL_PATH, 'wb') as file:
    pickle.dump(grid.best_estimator_,file)

"""**Zip and download from Colab**"""

!zip -r /content/heart.zip /content

from google.colab import files
files.download("/content/heart.zip")

"""# Test data

"""

#loading a nrew dataset to evaluate the model's performance
test = pd.read_csv(TEST_PATH)

test.head(10)

test_X = test.drop(['TRUE output','fbs'],axis=1)

true_y = test['TRUE output']

res = grid.predict(test_X)

test['predicted output'] = res

labels = ['No Heart Attack','Heart Attack']
cm = confusion_matrix(true_y,res)
cr = classification_report(true_y,res, target_names = labels)

print(cr)

"""### Notes 
1. The accuracy of this test dataset is 90%!
2. The recall of True Positive patient being 100%, which is the best news on our context as it would be dangerous to mark a False Positive patient.
"""

disp = ConfusionMatrixDisplay(confusion_matrix=cm,display_labels =labels)
disp.plot(cmap='BuGn')
plt.rcParams['figure.figsize'] = [7, 7]
plt.show()

print(classification_report)

test.head(10)