# -*- coding: utf-8 -*-
"""ha.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16JTDiMugEBOcGWaIA-Avn9XqLAowjDbc

# Heart Predict Prediction
- A classification problem to predict whether someone has heart attack
- [Source Data](https://www.kaggle.com/datasets/rashikrahmanpritom/heart-attack-analysis-prediction-dataset/code?sort=votes)

# Importing Libraries
"""

import os
import pickle
import numpy as np 
import pandas as pd
import seaborn as sns 
import scipy.stats as ss
import matplotlib.pyplot as plt

from sklearn.pipeline import Pipeline
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import \
    train_test_split, GridSearchCV
from sklearn.preprocessing import \
    LabelEncoder, MinMaxScaler, StandardScaler

from sklearn.linear_model import \
    LogisticRegression, LinearRegression

from sklearn.ensemble import \
    RandomForestClassifier, GradientBoostingClassifier

from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC ,SVR

from sklearn.metrics import \
    f1_score, confusion_matrix, accuracy_score, classification_report

from sklearn.metrics import ConfusionMatrixDisplay

#Cramers Function
def cramers_corrected_stat(confusion_matrix):
    """ calculate Cramers V statistic for categorial-categorial association.
        uses correction from Bergsma and Wicher,
        Journal of the Korean Statistical Society 42 (2013): 323-328
    """
    chi2 = ss.chi2_contingency(confusion_matrix)[0]
    n = confusion_matrix.sum()
    phi2 = chi2/n
    r,k = confusion_matrix.shape
    phi2corr = max(0, phi2 - ((k-1)*(r-1))/(n-1))  
    rcorr = r - ((r-1)**2)/(n-1)
    kcorr = k - ((k-1)**2)/(n-1)
    return np.sqrt(phi2corr / min( (kcorr-1), (rcorr-1)))

# constants 
## clean path!
CSV_PATH = os.path.join(os.getcwd(),'heart.csv')
BEST_ESTIMATOR_SAVE_PATH=os.path.join(os.getcwd(),'best_estimator.pkl')

def plot_con_graph(con_col,df,c):
  '''
  This function is meant to plot continuos data using seaborn distplot function

  Parameters
  ----------
  con : LIST
      con_col contains the name of the categorical columns.
  df : DATAFRAME
      DESCRIPTION
  c : str
      String that sets the colour of the plot

  Returns
  -------
  None.
  '''

  for i in con_col:
    plt.figure()
    sns.distplot(df[i],color='c')
    plt.show()

"""# 1) Data Loading"""

# constants
# from google.colab import drive
# drive.mount('/content/drive')

df = pd.read_csv(CSV_PATH)

df.head()

df.describe().T

"""Notes:
set:
caa : 4 = null
thal : 0 = null
"""

df.nunique()

#checking for missing values
df.isna().sum()

"""Obervation:

1. Age is measured in days, we might wanna convert that to year for eda, and check/drop cvd babies?
2. bmi can be added since obesity is (highly) corelated to CVD

# 2) Data Vizualization
"""

con = ['age','trtbps','chol','thalachh','oldpeak']
cat = df.drop(labels=con, axis=1).columns

custom = ['#FEBB9A','#B3D9FF', '#E3F8B9','#FFC0E7','#A9F3FF','#DFD8D1']

for i in cat:
  plt.figure()
  sns.countplot(df[i], palette= custom) 
  plt.show()

"""Notes:
slightly imbalance dataset, but acceptable
"""

df.groupby(['sex','output']).agg({'output':'count'}).plot(kind='bar',color= '#FEBB9A')
# df.groupby(['sex','alco']).agg({'alco':'count'}).plot(kind='bar',color= '#E3F8B9')
# df.groupby(['sex','smoke']).agg({'smoke':'count'}).plot(kind='bar',color= '#FFC0E7')
# df.groupby(['sex','active']).agg({'active':'count'}).plot(kind='bar',color= '#A9F3FF')

# 1: Male 0:Female

plot_con_graph(con,df,'c')

df.boxplot(figsize=(11,7))

df.describe().T

"""Observation:
1. age range focuses on 29-77 
2. thall & caa hahave some hidden null data
3. one outlier in chol

# 3) Data Cleaning
1. Outliers
2. Missing Values
3. Duplicated

## 3.1 Dealing with Outliers
1. Clip the only outliers from chol col
"""

#METHOD 2: clipping the outliers
#clipping data with realistic blood pressures
df['chol'] = df['chol'].clip(126,430)

df.boxplot(figsize=(10,6))
#outliers are now treated

"""## 3.2 Dealing with Missing Values

"""

#querying the data with invalid input

df.query('thall == 0 or caa == 4')

"""We have 7 data with positive outputs.
Since we have a really small datasets, we might want to make fill the datasets by the output. 
- to improve the performance
- drop it for the first round
"""

df.drop(df.query('thall == 0 or caa == 4').index,axis=0,inplace=True)

df.describe().T

# df.boxplot()

# for i in con:
#   df[i] = df[i].fillna(df[i].median())

# for i in cat:
#   df[i] = df[i].fillna(df[i].mode()[0])

# df.isna().sum()

"""##3.3 Droping Duplicates"""

df.duplicated().sum()

df.describe().T
#clean datasets of 296 entries

"""# 4) Features Selection"""

#cat = df[cat].drop(labels='output', axis=1).columns

for i in cat:
    print(i)
    matrix = pd.crosstab(df[i],df['output']).to_numpy()
    print(cramers_corrected_stat(matrix))

#only cholesterol has correlation with CVD

for i in con:
    print(i)
    lr=LogisticRegression()
    lr.fit(np.expand_dims(df[i],axis=-1),df['output'])
    print(lr.score(np.expand_dims(df[i],axis=-1),df['output']))

#checking why fbs output = 0
display(df.groupby('fbs')['fbs'].size())

"""Observation from EDA:
1. oldpeak, age, trrtbps, chol has high accuracy
2. 
"""

fig = plt.figure(figsize=(6,6))
gs = fig.add_gridspec(1,1)
gs.update(wspace=0.3, hspace=0.15)
ax0 = fig.add_subplot(gs[0,0])

df_corr = df[con].corr().transpose()

mask = np.triu(np.ones_like(df_corr))
ax0.text(1.5,-0.1,"Correlation Matrix",fontsize=22, fontweight='bold', fontfamily='serif', color="#000000")
df_corr = df[con].corr().transpose()
sns.heatmap(df_corr,mask=mask,fmt=".1f",annot=True,cmap='coolwarm')
plt.show()

"""# 5) Pre-processing"""

#select features that has the highest coleration
X = df.drop(['output'],axis=1)
y = df['output']

X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3,random_state=413)

"""# 6) Model Development"""

#Pipeline to find the best model
#Logistic Regression
pipeline_mms_lr = Pipeline([
                           ('Min_Max_Scaler', MinMaxScaler()),
                           ('Logistic_Classifier', LogisticRegression())
                           ]) # Pipeline([STEPS])


pipeline_ss_lr = Pipeline([
                           ('Standard_Scaler',StandardScaler()), 
                           ('Logistic_Classifier', LogisticRegression())
                           ]) 

#Random Forest
pipeline_mms_dt = Pipeline([
                           ('Min_Max_Scaler', MinMaxScaler()),
                           ('Decision_Tree_Classifier', DecisionTreeClassifier())
                           ]) 


pipeline_ss_dt = Pipeline([
                           ('Standard_Scaler',StandardScaler()), 
                           ('Decision_Tree_Classifier', DecisionTreeClassifier())
                           ]) 

#SVC
pipeline_mms_svc = Pipeline([
                            ('Min_Max_Scaler', MinMaxScaler()),
                            ('SVC_Classifier', SVC())
                            ]) 


pipeline_ss_svc = Pipeline([
                            ('Standard_Scaler', StandardScaler()),
                            ('SVC_Classifier', SVC())
                            ])
#gradient boost
pipeline_mms_gb = Pipeline([
                            ('Min_Max_Scaler',MinMaxScaler()),
                            ('GBoost_Classifier', GradientBoostingClassifier())
                            ])

pipeline_ss_gb = Pipeline([
                            ('Min_Max_Scaler',StandardScaler()),
                            ('GBoost_Classifier', GradientBoostingClassifier())
                            ])


#storing pipelines in a list
pipelines = [pipeline_mms_lr, pipeline_ss_lr,
             pipeline_mms_dt, pipeline_ss_dt, 
             pipeline_mms_svc, pipeline_ss_svc,
             pipeline_mms_gb, pipeline_ss_gb]

#fitting of data
for pipe in pipelines:
    pipe.fit(X_train, y_train)

#create a list that store model name for reporting table, to be looped and parsed as keys
model_name = ['MMS+Logistic Regression', 'SS+Logistic Regression', 
              'MMS+Decision Tree', 'SS+Decision Tree',
              'MMS+SVC','SS+SVC',
              'MMS+Gradient Boost','SS+Gradient Boost',
              ]

#dictionary to store evaluation matrics of all the pipelines
pipe_dict =  {}

best_score = 0

#model evaluation
for i, model in enumerate(pipelines):   
    y_pred = model.predict(X_test)

    pipe_dict[model_name[i]] = [accuracy_score(y_test,y_pred),
                    f1_score(y_test,y_pred, average='weighted')]

    if model.score(X_test, y_test) > best_score:
        best_score = model.score(X_test, y_test)
        best_pipeline = model_name[i]

print('The best scaling approach for Cardiovascular Disease Prediction is {} with the accuracy of {}'.
      format(best_pipeline,best_score))

print(pipe_dict)
#print(pipe_dict.keys(2)) duh! can't access key by index

"""# Model Comparison Table"""

#model comparison tables
model_comparison_df = pd.DataFrame.from_dict(pipe_dict).T
model_comparison_df.columns = ['Accuracy', 'F1 Score']
model_comparison_df = model_comparison_df.sort_values('F1 Score', ascending=False)
model_comparison_df.style.background_gradient(cmap='coolwarm')
#model_comparison_df.sort_values(by='F1 Score',ascending=False).style.highlight_max()

"""
### Gridsearch cv

"""

from sklearn.model_selection import GridSearchCV
#brute force to try out all combinations
pipeline_ss_lr = Pipeline([
                           ('Standard_Scaler',StandardScaler()), 
                           ('Logistic_Classifier', LogisticRegression())
                           ])

pipeline_ss_lr.get_params().keys()
#printing all hyperparameters for reference

grid_param = [{'Logistic_Classifier__C':[1.0,1.5,2.5],
               'Logistic_Classifier__class_weight':[None,'balanced'],
               'Logistic_Classifier__max_iter':[100,500,1000],
               'Logistic_Classifier__random_state':[1,3,4],
               'Logistic_Classifier__solver':['lbfgs','liblinear','sag']
             }]

grid_search = GridSearchCV(pipeline_ss_lr, grid_param, 
                                 cv=5, verbose=1, n_jobs=-1)

grid = grid_search.fit(X_train, y_train)
print(grid_search.score(X_test,y_test))
display(grid.best_params_)

#Model Evaluation
y_pred = grid.predict(X_test)
y_true = y_test

labels = ['No Heart Attack','Heart Attack']
cm = confusion_matrix(y_true,y_pred)
cr = classification_report(y_true,y_pred, target_names = labels)

print(cr)

disp = ConfusionMatrixDisplay(confusion_matrix=cm,display_labels =labels)
disp.plot(cmap='RdPu')
plt.rcParams['figure.figsize'] = [7, 7]
plt.show()

print(classification_report)

#saving the best model
BEST_ESTIMATOR_SAVE_PATH= os.path.join(os.getcwd(),'best_estimator.pkl')

with open(BEST_ESTIMATOR_SAVE_PATH, 'wb') as file:
    pickle.dump(grid.best_estimator_,file)

"""**Zip and download from Colab**"""

!zip -r /content/heart.zip /content

from google.colab import files
files.download("/content/heart.zip")

"""# 7 Streamlit

Writing app.py file
"""

# Commented out IPython magic to ensure Python compatibility.
# %%writefile app.py
# 
# import os
# import pickle
# import numpy as np
# import streamlit as st
# import pandas as pd
# 
# 
# #%% deployment
# MODEL_PATH = os.path.join(os.getcwd(),'best_estimator.pkl')
# 
# with open(MODEL_PATH,'rb') as file:
#     model = pickle.load(file)
# 
# # Cholesterol 1,2,3
# # ap_hi : 
# # ap_lo
# # new_data = [cholesterol,ap_hi,ap_lo]
# # new_data = np.expand_dims([2,200,110],axis=0)
# # outcome = model.predict(new_data)[0]
# 
# 
# with st.form("Heart Attack Predictor App"):
#     chol = st.selectbox('Cholesterol Level: 1: normal, 2: above normal, 3: well above normal', (1,2,3))
#     sys = st.number_input('Systolic BP')
#     dia = st.number_input('Diastolic BP')
# 
#     # Every form must have a submit button.
#     submitted = st.form_submit_button("Submit")
#     if submitted:
#         new_data = np.expand_dims([chol,sys,dia],axis=0)
#         outcome = model.predict(new_data)[0]
#         
#         if outcome == 0:
#             st.write('Congrats you are healthy, keep it up')
#             st.balloons()
#         else:
#             st.write('Start Exercising now!!!')
#             st.snow()
# 
#